# Comprehensive Review: DevAC/Vivief Vision, Concepts, and Implementation

> A thorough analysis of the DevAC vision, current implementation, comparison with similar tools, and recommendations for next steps.

**Date**: 2026-01-21
**Status**: Strategic Review Document (Updated)

---

## Executive Summary

DevAC/Vivief represents an ambitious attempt to create a **unified, queryable development platform** where code behavior, architecture, and development workflow become deterministic data. The vision is coherent and architecturally sound, with significant implementation progress since the last review.

**Key Finding**: DevAC's core value proposition‚Äîmaking code effects queryable via SQL‚Äîremains unique in the ecosystem. No other tool combines code property graphs, effect extraction, rules-based aggregation, multi-repo federation, and MCP integration in a single coherent framework.

**Update (2026-01-21)**: Major progress since last review:
- ‚úÖ **CLI v4.0 Reorganization**: 50+ commands consolidated into 3 core commands (sync, status, query)
- ‚úÖ **Browser Automation Suite**: v0.2.0 released (browser-cli, browser-core, browser-mcp)
- ‚úÖ **8 New ADRs**: Browser automation architecture (0035-0039), CLI/MCP reorganization (0040-0042)
- ‚úÖ **Hook-Based Automation**: Complete with hooks.json, `--inject`, `--on-stop` flags (Phase 3)
- ‚úÖ **Version**: Now at v0.27.0 across packages

**Previously Complete** (Phases 0-3):
- ‚úÖ Phase 0: JSX extraction (40 tests, RENDERS/PASSES_PROPS edges)
- ‚úÖ Phase 1: A11y attributes (ARIA relationship edges, element ID tracking)
- ‚úÖ Phase 2: WCAG validation (5 rules, 73 tests, unified diagnostics)
- ‚úÖ Phase 3: Hook-based validation triggering (hooks.json, E2E tests)

**Quality Score**: 8.5/10 (up from 8/10) ‚Äî CLI reorganization significantly improves usability

**Important**: This document is to be updated by further reviews on review requests.

### The proposed prompt to update review is based on the below prompt creating the first version:

do a very thorough review of the total vivief documentation with its vision and concepts 
and the current implementation (code, docs, adr)and determine the pros and cons and the quality of the total concept and vision 
and compare it against other initiatives that do similar thing 
and compare vivief against them this to enable design choices and next steps
the previous time we did a comprehensive review we created docs/spec/comprehensive-review.md and docs/spec/gaps.md, use them as the documents to update with the updated comprehensive review and new gaps.


### The first version (2026-01-17) is created from:

do a very thorough review of the total documentation with its vision and concepts 
and the current implementation and determine the pros and cons and the quality of the total concept and vision 
and compare it against other initiatives that do similar thing 
and compare vivief against them this to enable design choices and next steps

---

## Part 1: Vision Quality Assessment

### 1.1 Core Thesis Evaluation

**The Thesis**: 
> "By making all relevant state queryable and modeling development as state machines with effects, we enable humans and LLMs to collaborate productively."

**Strengths**:
- **Unifying Abstraction**: `effectHandler = (state, effect) => (state', [effect'])` elegantly captures HTTP, state machines, validation, and reasoning
- **Clear Boundary**: Deterministic (systems) vs non-deterministic (humans/LLMs) is a practical division
- **Future-Proof**: Designed for LLM capabilities to improve without changing foundations
- **Honest**: Documentation explicitly acknowledges what's NOT implemented (rare and valuable)

**Weaknesses**:
- **Ambitious Scope**: Trying to unify code analysis, validation, workflow, and documentation generation
- **Implementation Gap**: Vision docs describe capabilities that don't exist yet (Actors, OTel)
- **Complexity Ceiling**: The Four Pillars + Analytics Layer + Effects + Actors + UI Effects creates cognitive load

### 1.2 Conceptual Coherence

| Concept | Coherence | Issue |
|---------|-----------|-------|
| Effects as universal abstraction | ‚úÖ Strong | Well-defined, extensible |
| Four Pillars model | ‚úÖ Strong | Clear separation of concerns |
| Seeds (queryable state) | ‚úÖ Strong | DuckDB + Parquet is solid choice |
| Hub federation | ‚úÖ Strong | Three-layer model is well-designed |
| JSX/UI extraction | ‚úÖ Strong | Phase 0-1 complete, 54+ tests |
| WCAG validation | ‚úÖ Strong | Phase 2 complete, 5 rules, 73 tests |
| Hook-based automation | ‚úÖ Strong | Phase 3 complete: hooks.json, CLI flags, E2E tests |
| **CLI Command Structure** | ‚úÖ Strong | **NEW**: 3-command model (sync/status/query) is elegant |
| **Browser Automation** | ‚úÖ Strong | **NEW**: 5 ADRs defining clean architecture |
| Actor model | üü° Medium | Vision clear, implementation pending |
| Rules Engine | üü° Medium | Works but no sequence matching |
| Effect correlation | ‚¨ú Weak | OTel integration not implemented |

### 1.3 Documentation Quality

**Excellent**:
- `foundation.md` (comprehensive, well-structured)
- `concepts.md` (clear glossary, quick reference)
- `gaps.md` (honest gap tracking)
- **47 ADRs** documenting design decisions (up from 43)
- **ADR-0041** (CLI v4.0 Reorganization) ‚Äî exemplary decision documentation

**Good**:
- `actors.md` (clean vision, no implementation confusion)
- `ui-effects.md` (clear aspirational model)
- `test-strategy.md` (explicit test types and their role)
- **Browser automation ADRs** (0035-0039) ‚Äî clean architecture decisions

**Needs Work**:
- Some vision docs reference unimplemented features as if they exist
- Cross-references between docs could be stronger
- Missing "How to extend" guides

---

## Part 2: Implementation Assessment

### 2.1 What Works Well

| Component | Status | Quality |
|-----------|--------|---------|
| TypeScript/JS parsing | ‚úÖ Production | 54KB parser, semantic analysis, type resolution |
| Python parsing | ‚úÖ Production | Subprocess-based, AST extraction |
| C# parsing | ‚úÖ Production | Tree-sitter based, 46KB |
| Effect extraction | ‚úÖ Production | FunctionCall, Store, Retrieve, Send, Request, Response |
| JSX extraction | ‚úÖ Production | RENDERS, PASSES_PROPS edges; 40 tests |
| A11y extraction | ‚úÖ Production | ARIA refs, element IDs, tabIndex; 14 tests |
| WCAG validation | ‚úÖ Production | 5 rules, 73 tests, unified diagnostics |
| Rules Engine | ‚úÖ Production | Domain categorization, 13KB builtin rules |
| Hub federation | ‚úÖ Production | DuckDB central, IPC concurrency (ADR-0024) |
| MCP Server | ‚úÖ Production | 22 tools, comprehensive coverage |
| **CLI v4.0** | ‚úÖ Production | **NEW**: 3 core commands (sync/status/query) + 47 total |
| Validation pipeline | ‚úÖ Production | tsc, eslint, test, coverage, WCAG integration |
| C4 diagram generation | ‚úÖ Production | Context, Container, Component levels |
| Hook-based automation | ‚úÖ Production | hooks.json, `--inject`, `--on-stop` flags (Phase 3) |
| **Browser automation** | ‚úÖ Production | **v0.2.0**: browser-cli, browser-core, browser-mcp |

### 2.2 Critical Gaps

| Gap | Vision Claims | Reality | Impact |
|-----|---------------|---------|--------|
| ~~JSX extraction~~ | ~~Query React components, A11y~~ | ‚úÖ **Complete** | Now queryable |
| ~~CLI complexity~~ | ~~Simple mental model~~ | ‚úÖ **Complete** (v4.0) | 3 core commands |
| **State machines** | Discover XState, infer from effects | Not implemented | Cannot query workflow state |
| **Effect correlation** | Match static to runtime via OTel | Not implemented | Cannot validate test coverage |
| **Sequence rules** | Match effect sequences | Not implemented | Cannot express "A then B" |
| ~~Hook-based triggers~~ | ~~Auto-inject diagnostics~~ | ‚úÖ **Complete** (Phase 3) | Proactive validation |
| **Scale benchmarking** | Handle large codebases | No benchmark data | Cannot compare to Augment (400k files) |
| **Language coverage** | Multi-language support | 3 languages | Aider supports 40+; limited reach |
| **Embeddings/RAG** | Semantic code search | SQL only | No vector-based similarity search |

### 2.3 Implementation Depth Analysis

```
Depth of Implementation by Area (Updated 2026-01-21):

Code Extraction     ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë 85%
  - Languages       ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 100% (TS, Py, C#)
  - Semantic        ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë 80% (types, imports)
  - UI/JSX          ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë 95%
  - State machines  ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 0%

Effects & Rules     ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 65%
  - Basic effects   ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 100%
  - Domain rules    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 100%
  - WCAG rules      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 100%
  - Sequence rules  ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 0%
  - Actor rules     ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 0%

Storage & Query     ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 95%
  - Seeds (Parquet) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 100%
  - Hub (DuckDB)    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 100%
  - Federation      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë 85%

CLI & UX            ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 98% (‚Üë from 85%)
  - Command structure ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 100% (v4.0 reorganization)
  - MCP tools       ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 100%
  - Plugin system   ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë 95%

Validation          ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë 90%
  - Type/lint/test  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 100%
  - Coverage        ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 100%
  - A11y/WCAG       ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 100%

Browser Automation  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë 90% (NEW)
  - Session mgmt    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 100% (ADR-0036)
  - Element refs    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 100% (ADR-0035)
  - Error handling  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 100% (ADR-0038)
  - E2E integration ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 60%

Runtime Integration ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 5%
  - OTel setup      ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 0%
  - Test spans      ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 0%
  - Correlation     ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 0%

Documentation Gen   ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 60%
  - C4 diagrams     ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 100%
  - API docs        ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 0%
  - State machines  ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 0%
```

---

## Part 3: Comparative Analysis

### 3.1 Similar Tools Landscape (Updated 2026-01-21)

The AI coding assistant market has undergone dramatic transformation. Key themes include the rise of **agentic capabilities**, the standardization of **Model Context Protocol (MCP)**, massive funding rounds, and the commoditization of basic AI coding features. AI coding tools have reached mass adoption with **84% of developers using them** and approximately **41% of commits being AI-assisted**.

| Tool | Focus | Approach | Overlap with DevAC |
|------|-------|----------|-------------------|
| **Joern** | Security analysis | Code Property Graph (CPG) | Graph model, static analysis |
| **Sourcegraph Cody/Amp** | Code intelligence | SCIP indexing + AI | Cross-repo search, semantic nav |
| **Nx** | Monorepo orchestration | Dependency graph | Affected analysis, caching |
| **Structurizr** | Architecture diagrams | C4 model DSL | Diagrams from code |
| **CodeRabbit** | AI code review | AST + AI | PR analysis, fix suggestions |
| **Augment Code** | AI assistant | 5-layer semantic comprehension | Context awareness, 400k+ files |
| **Cursor** | AI IDE | Custom models + Merkle sync | $29.3B valuation, background agents |
| **Windsurf** | AI IDE | Cascade AI + parallel agents | SWE-1.5 model, JetBrains support |
| **Claude Code** | CLI/SDK | Multi-agent development | Agent SDK, session teleportation |
| **Devin AI** | Autonomous agent | Full autonomy | $10.2B valuation, 25% of own PRs |
| **GitHub Copilot** | IDE extension | Agent Mode + MCP | Skills system, GPT-5.1/Claude Opus 4.5 |
| **Aider** | AI pair programmer | Tree-sitter + PageRank | 40+ languages, terminal-native |

### 3.2 Detailed Comparisons

#### DevAC vs Joern (Code Property Graph)

| Aspect | DevAC | Joern |
|--------|-------|-------|
| **Primary Use** | Development productivity | Vulnerability detection |
| **Graph Model** | Effects + Seeds (Parquet) | CPG (AST + CFG + PDG) |
| **Query Language** | SQL (DuckDB) | Scala DSL (Gremlin-based) |
| **Taint Analysis** | ‚ùå Not implemented | ‚úÖ Full support |
| **Multi-repo** | ‚úÖ Hub federation | ‚ùå Single project |
| **MCP Integration** | ‚úÖ 22 tools | ‚ùå None |
| **Build Required** | ‚ùå No | ‚ùå No (unlike CodeQL) |
| **Language Support** | TS, Python, C# | C/C++, Java, PHP, JS |

**Verdict**: Joern is more mature for security analysis; DevAC targets developer productivity and AI collaboration. Complementary, not competing.

#### DevAC vs Sourcegraph/Cody/Amp

| Aspect | DevAC | Sourcegraph |
|--------|-------|-------------|
| **Primary Use** | Queryable code effects | Code search + navigation |
| **Indexing Format** | Seeds (Parquet) | SCIP (Protobuf) |
| **Cross-repo** | ‚úÖ Hub | ‚úÖ Native |
| **AI Integration** | ‚úÖ MCP server | ‚úÖ Cody assistant + Amp |
| **Effect Extraction** | ‚úÖ Comprehensive | ‚ùå Symbols only |
| **Architecture Docs** | ‚úÖ C4 generation | ‚ùå Manual |
| **Validation** | ‚úÖ Unified diagnostics | ‚ùå Not integrated |
| **Pricing** | Open source | $$$ Enterprise |

**Verdict**: Sourcegraph excels at code search and navigation at scale. DevAC goes deeper into *behavior* (effects, rules, architecture). Amp validates the market for LLM code context.

#### DevAC vs Nx

| Aspect | DevAC | Nx |
|--------|-------|-----|
| **Primary Use** | Code understanding | Build orchestration |
| **Affected Analysis** | ‚úÖ Symbol-level | ‚úÖ Project-level |
| **Caching** | ‚úÖ Content-hash seeds | ‚úÖ Computation caching |
| **Dependency Graph** | ‚úÖ Call-level | ‚úÖ Package-level |
| **Task Running** | ‚ùå Not a build tool | ‚úÖ Core feature |
| **Cloud** | ‚ùå Local hub | ‚úÖ Nx Cloud |
| **Effect Extraction** | ‚úÖ What code does | ‚ùå What depends on what |

**Verdict**: Nx solves "what to rebuild"; DevAC solves "what code does". Could integrate: Nx for build, DevAC for code intelligence.

#### DevAC vs Structurizr (C4)

| Aspect | DevAC | Structurizr |
|--------|-------|-------------|
| **Diagram Source** | Extracted from code | Manually authored DSL |
| **Model Location** | Seeds (derived) | DSL file (authoritative) |
| **C4 Levels** | Context, Container, Component | All 4 levels + deployment |
| **Automation** | ‚úÖ Auto-generated | ‚ùå Manual maintenance |
| **Accuracy** | What code does (may drift) | What you say (may be stale) |
| **Integration** | Effects ‚Üí C4 | Standalone |

**Verdict**: Structurizr is mature for intentional architecture documentation. DevAC generates diagrams from reality but may include noise.

#### DevAC vs AI Coding Assistants (Cursor, Windsurf, Claude Code)

| Aspect | DevAC | AI IDEs/Assistants |
|--------|-------|-------------------|
| **Primary Use** | Queryable code intelligence | AI-powered development |
| **Architecture** | CLI + MCP server | Full IDE with custom models |
| **Context Source** | Deterministic seeds | RAG / embeddings / Merkle trees |
| **Query Interface** | SQL + MCP | Natural language |
| **Transparency** | Fully queryable | Black box reasoning |
| **Determinism** | ‚úÖ Same query = same result | ‚ùå LLM variability |
| **Background Agents** | ‚ùå | ‚úÖ (Cursor 2.0, Windsurf) |

**Verdict**: AI assistants are *consumers* of code intelligence. DevAC is a *provider*. They're complementary: DevAC provides the deterministic substrate that AI assistants need.

#### DevAC vs GitHub Copilot (2026 State)

| Aspect | DevAC | GitHub Copilot |
|--------|-------|----------------|
| **Primary Use** | Queryable code effects | AI coding assistance |
| **Agent Capabilities** | MCP tools | Agent Mode with Skills system |
| **MCP Support** | ‚úÖ Native (22 tools) | ‚úÖ Adopted March 2025 |
| **Effect Extraction** | ‚úÖ Comprehensive | ‚ùå None |
| **Custom Workflows** | Plugin system | Skills folders (instructions + scripts) |
| **Cross-agent Memory** | ‚ùå | ‚úÖ Learning across tools |
| **Models** | Uses Claude via MCP | GPT-5.1, Claude Opus 4.5, Gemini 3 Pro |

**Verdict**: Copilot's Skills system shows the value of deterministic + LLM combination. DevAC could serve as a Copilot MCP server, providing effect-based context.

#### DevAC vs GitHub Stack Graphs

| Aspect | DevAC | Stack Graphs |
|--------|-------|--------------|
| **Primary Use** | Queryable code effects | Code navigation at scale |
| **Scale Target** | Hundreds of repos | Petabyte-scale (all of GitHub) |
| **Indexing** | Package-incremental seeds | File-incremental DSL |
| **Name Resolution** | Import tracking, type inference | Stack-based scope matching |
| **Query Language** | SQL (DuckDB) | Graph traversal DSL |
| **Effect Extraction** | ‚úÖ Behavior as data | ‚ùå Structure only |
| **MCP Integration** | ‚úÖ 22 tools | ‚ùå None |
| **Open Source** | ‚úÖ Yes | ‚úÖ Yes |

**Verdict**: Stack Graphs solves precise name resolution at massive scale. DevAC solves *what code does*, not just what it references.

#### DevAC vs Devin AI

| Aspect | DevAC | Devin AI |
|--------|-------|----------|
| **Primary Use** | Code intelligence provider | Autonomous AI engineer |
| **Autonomy Level** | Tools for AI | Full task autonomy |
| **Typical Task** | Query code behavior | Multi-hour development tasks |
| **Human Oversight** | Query-based interaction | Periodic checkpoints |
| **Best For** | Real-time code context | Tasks requiring 4-8 hours of junior engineer time |
| **Adoption** | Open source | Enterprise (Goldman Sachs, Palantir) |

**Verdict**: Devin represents the far end of AI autonomy. DevAC provides the deterministic foundation that autonomous agents need for accurate code understanding.

#### DevAC vs Aider Repo Map

| Aspect | DevAC | Aider |
|--------|-------|-------|
| **Primary Use** | Queryable code effects | AI pair programming |
| **Language Support** | 3 (TS, Python, C#) | 40+ via Tree-sitter |
| **Context Building** | Seeds (full extraction) | Repo map (PageRank priority) |
| **Graph Model** | Nodes, edges, effects | Tags (symbol ‚Üí file map) |
| **A11y/WCAG** | ‚úÖ Full support | ‚ùå None |
| **Terminal Native** | ‚úÖ CLI | ‚úÖ CLI |
| **Open Source** | ‚úÖ Yes | ‚úÖ Yes |

**Verdict**: Aider's 40+ language support and PageRank-based context selection are impressive. DevAC's deeper extraction (effects, WCAG) suits specialized use cases.

#### DevAC vs CodeRabbit

| Aspect | DevAC | CodeRabbit |
|--------|-------|------------|
| **Primary Use** | Queryable code intelligence | AI-powered code review |
| **Graph Model** | Effects + Seeds (Parquet) | AST + code graph |
| **Diagnostics** | ‚úÖ Unified (5 sources + WCAG) | PR-focused |
| **WCAG/A11y** | ‚úÖ Integrated | ‚ùå None |
| **PR Integration** | ‚ùå CLI/MCP | ‚úÖ Native |
| **Auto-fix** | Via Claude + MCP | ‚úÖ Built-in |

**Verdict**: CodeRabbit shows value of code graph + AI for reviews. DevAC's unified diagnostics (including WCAG) is broader.

### 3.3 Feature Comparison Matrix (Updated 2026-01-21)

| Feature | DevAC | Joern | Stack Graphs | Cursor | Copilot | Aider | CodeRabbit |
|---------|-------|-------|--------------|--------|---------|-------|------------|
| **Effect extraction** | ‚úÖ | ‚ùå | ‚ùå | ‚ùå | ‚ùå | ‚ùå | ‚ùå |
| **SQL queries** | ‚úÖ | ‚ùå | ‚ùå | ‚ùå | ‚ùå | ‚ùå | ‚ùå |
| **MCP server** | ‚úÖ | ‚ùå | ‚ùå | ‚ùå | ‚úÖ | ‚ùå | ‚ùå |
| **Cross-repo** | ‚úÖ | ‚ùå | ‚úÖ | ‚úÖ | ‚úÖ | ‚ùå | ‚úÖ |
| **A11y/WCAG** | ‚úÖ | ‚ùå | ‚ùå | ‚ùå | ‚ùå | ‚ùå | ‚ùå |
| **C4 diagrams** | ‚úÖ | ‚ùå | ‚ùå | ‚ùå | ‚ùå | ‚ùå | ‚ùå |
| **Hook automation** | ‚úÖ | ‚ùå | ‚ùå | ‚ùå | ‚úÖ | ‚ùå | ‚ùå |
| **Browser automation** | ‚úÖ | ‚ùå | ‚ùå | ‚ùå | ‚ùå | ‚ùå | ‚ùå |
| **Background agents** | ‚ùå | ‚ùå | ‚ùå | ‚úÖ | ‚úÖ | ‚ùå | ‚ùå |
| **IDE integration** | ‚ùå | ‚ùå | ‚úÖ | ‚úÖ | ‚úÖ | ‚ùå | ‚úÖ |
| **40+ languages** | ‚ùå | ‚ùå | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ |
| **Taint analysis** | ‚ùå | ‚úÖ | ‚ùå | ‚ùå | ‚ùå | ‚ùå | ‚ùå |
| **Vector/RAG** | ‚ùå | ‚ùå | ‚ùå | ‚úÖ | ‚úÖ | ‚ùå | ‚ùå |
| **3-command CLI** | ‚úÖ | ‚ùå | ‚ùå | N/A | N/A | ‚ùå | N/A |

### 3.4 Unique Value Proposition

**What DevAC does that no other tool does** (updated 2026-01-21):

1. **Effects as first-class citizens**: No other tool extracts code *behavior* (FunctionCall, Store, Retrieve, Send) as queryable data
2. **Rules-based aggregation**: Compose low-level effects into domain effects (ChargePayment from DB + HTTP + Email)
3. **Unified diagnostics**: Type errors, lint, test, coverage, CI, issues, PR reviews, **WCAG** in one queryable table
4. **MCP-native**: Built for AI assistants from the ground up (22 tools)
5. **Federation with DuckDB**: Cross-repo queries without central server overhead
6. **WCAG validation as diagnostics**: A11y issues alongside type/lint errors‚Äîno other tool integrates WCAG into a unified diagnostics pipeline
7. **Hook + Instructions pattern**: Reliable injection via hooks (`--inject`, `--on-stop`), intelligent action via instructions
8. **Browser automation suite**: Playwright-based browser control via CLI and MCP for E2E validation
9. **Three-command CLI** (NEW): `sync`, `status`, `query` ‚Äî elegant mental model vs 50+ fragmented commands

---

## Part 4: Strengths and Weaknesses

### 4.1 Strengths

| Strength | Evidence | Impact |
|----------|----------|--------|
| **Coherent vision** | Foundation doc, 47 ADRs | Reduces architectural confusion |
| **Solid storage layer** | DuckDB + Parquet working | Fast queries, easy federation |
| **Effect taxonomy** | 6 data + 2 flow + 3 group effects | Comprehensive behavior model |
| **Rules engine** | Domain + WCAG classification | Enables C4 generation + A11y |
| **Honest gap tracking** | gaps.md exists and is accurate | Trust in documentation |
| **MCP integration** | 22 tools production-ready | AI-first architecture |
| **Multi-language** | TS, Python, C# | Practical polyglot support |
| **JSX/A11y extraction** | 54+ tests, RENDERS/PASSES_PROPS | Can analyze React component hierarchy |
| **WCAG validation** | 5 rules, 73 tests | A11y as first-class diagnostics |
| **Hook-based automation** | hooks.json, `--inject`, `--on-stop` | Proactive validation workflow |
| **Browser automation** | v0.2.0 suite with 5 ADRs | E2E validation capability |
| **CLI v4.0** (NEW) | 3 core commands, ADR-0041 | Significantly improved usability |
| **Validation infrastructure** | 74 test files, 18,842 lines | Mature diagnostics pipeline |

### 4.2 Weaknesses

| Weakness | Evidence | Impact |
|----------|----------|--------|
| **Vision-implementation gap** | Actors at 0%, OTel at 0% | Still significant for advanced features |
| **No runtime correlation** | OTel not integrated | Cannot validate test coverage |
| **Single-file rules** | No sequence matching | Cannot express "A then B" |
| **Limited adoption signals** | No public usage stats | Unclear product-market fit |
| **No scale benchmarks** | No data vs Augment 400k files | Cannot prove large codebase support |
| **Limited language coverage** | 3 languages vs Aider's 40+ | Narrower applicability |
| **No IDE integration** | CLI/MCP only | Higher adoption friction vs Cursor |
| **No background agents** (NEW) | Unlike Cursor/Windsurf/Copilot | Cannot run autonomous tasks |

### 4.3 Risks

| Risk | Likelihood | Mitigation |
|------|------------|------------|
| **Scope creep** | High | Focus on core extraction + rules |
| **Vision drift** | Medium | Keep vision docs stable, iterate implementation |
| **Performance at scale** | Medium | DuckDB handles millions of rows; test at 1M+ nodes |
| **Maintenance burden** | Medium | Focus on TS, Python; community for others |
| **Obsolescence** | Low | Effects model is fundamental, not tied to current LLMs |
| **Market commoditization** (NEW) | Medium | AI coding tools commoditizing fast; differentiate on depth |

---

## Part 5: Strategic Recommendations

### 5.1 Prioritized Next Steps (Updated 2026-01-21)

**Completed Phases** ‚úÖ

| Phase | Status | Key Deliverables |
|-------|--------|------------------|
| Phase 0: JSX Extraction | ‚úÖ Complete | 40 tests, RENDERS edges |
| Phase 1: A11y Attributes | ‚úÖ Complete | ARIA refs, element IDs, tabIndex |
| Phase 2: WCAG Validation | ‚úÖ Complete | 5 rules, 73 tests |
| Phase 3: Hook-Based Automation | ‚úÖ Complete | hooks.json, `--inject`, `--on-stop` |
| CLI v4.0 Reorganization | ‚úÖ Complete | sync/status/query + ADR-0041 |
| Browser Automation v0.2.0 | ‚úÖ Complete | 3 packages + 5 ADRs |

**Phase 4A: OTel Integration (NEXT PRIORITY)**

| Task | Priority | Effort | Impact |
|------|----------|--------|--------|
| OTel SDK integration | P1 | Medium | Foundation for correlation |
| Test span exporter | P1 | Medium | Captures runtime effects |
| Correlation queries | P2 | Medium | "Which effects are tested?" |

**Phase 4B: Effect Correlation**

| Task | Priority | Effort | Impact |
|------|----------|--------|--------|
| OTel spans table | P1 | Medium | Store runtime data |
| Correlation view | P2 | Medium | Join effects and spans |
| Coverage queries | P2 | Medium | "Which effects are tested?" |

**Phase 4C: Effect Probing (Novel Concept)**

| Task | Priority | Effort | Impact |
|------|----------|--------|--------|
| Entity ID injection | P2 | Medium | Link spans to code |
| Black-box + tracing | P2 | High | Validate without code changes |
| Sequence validation | P3 | Medium | Verify effect order |

**Phase 5: Actor Discovery**

| Task | Priority | Effort | Impact |
|------|----------|--------|--------|
| XState v5 extraction | P2 | High | Explicit state machines |
| Effect sequence rules | P2 | High | Enables inference |
| Actor effect type | P2 | Medium | Queryable state machines |

**Phase 6: Scalability & Competitive Parity**

| Task | Priority | Effort | Impact |
|------|----------|--------|--------|
| Scale benchmarking (100k+ nodes) | P2 | Medium | Prove large codebase support |
| Memory profiling | P2 | Low | Optimize for constrained envs |
| Language coverage research | P3 | Low | Roadmap for more languages |
| Embeddings exploration | P3 | High | Semantic search capability |

### 5.2 What NOT to Do (Updated)

1. **Don't build a full IDE** ‚Äî Stay focused on queryable intelligence (use MCP for IDE integration)
2. **Don't compete with Nx** ‚Äî Complement, don't replace build tools
3. **Don't add more languages before OTel** ‚Äî Runtime correlation is more valuable than breadth
4. **Don't build production OTel before test OTel** ‚Äî Start with controlled environment
5. **Don't chase scale before proving value** ‚Äî Augment has 400k files, but DevAC's value is depth
6. **Don't add embeddings without clear use case** ‚Äî SQL queries are deterministic; RAG adds variability
7. **Don't build background agents** (NEW) ‚Äî Let Cursor/Copilot handle autonomy; focus on intelligence provision

### 5.3 Partnership Opportunities (Updated 2026-01-21)

| Partner | Integration | Value |
|---------|-------------|-------|
| **Sourcegraph** | SCIP indexing + DevAC effects | Best-of-both code intelligence |
| **Nx** | DevAC analysis in Nx Cloud | Deeper affected analysis |
| **Storybook** | Effects in Storybook addon | Unified component docs |
| **Anthropic/Claude** | Official DevAC MCP | Better AI coding experience |
| **Claude Code Hooks** | Native hook integration | First-class validation automation |
| **Cursor** (NEW) | DevAC as MCP provider | Effect-based context for $29B IDE |
| **GitHub Copilot** (NEW) | DevAC in Skills system | Effects + WCAG for Copilot |
| **CodeRabbit** (NEW) | DevAC diagnostics for reviews | Unified diagnostics in PRs |

### 5.4 Success Metrics (Updated 2026-01-21)

| Metric | Previous | Current | Target (6 mo) | Target (12 mo) |
|--------|----------|---------|---------------|----------------|
| Node kinds extracted | 19 | **19** | 22 (+ Actor kinds) | 25 |
| Edge types | 10 | **10** | 12 (+ OTel) | 15 |
| WCAG rules | 5 | **5** | 10 | 15 |
| Test files | 74 | **74** | Maintain | Maintain |
| CLI commands | 47 | **47** (reorganized) | 50 | 55 |
| MCP tools | 22 | **22** | 25 | 30 |
| ADRs | 43 | **47** (+4) | As needed | As needed |
| Phases complete | 4 | **4** | 5 (+ OTel) | 6 (+ Actors) |
| Hub query latency (p95) | ~200ms | ~200ms | <150ms | <100ms |
| Scale benchmark | None | None | 100k nodes tested | 500k nodes tested |
| Version | 0.27.0 | **0.27.0** | 0.30.0 | 1.0.0 |

**Competitive Benchmarks**:
| Competitor | Their Scale/Capability | DevAC Target |
|------------|------------------------|--------------|
| Augment | 400k+ files | Benchmark at 100k files |
| Stack Graphs | Petabyte | Focus on depth, not breadth |
| Aider | 40+ languages | 5 languages (TS, Python, C#, Go, Rust) |
| Cursor | $29.3B, background agents | MCP provider integration |
| Copilot | Skills system, cross-agent memory | Effects-based skill |

---

## Part 6: Conclusion

### 6.1 Overall Assessment (Updated 2026-01-21)

DevAC/Vivief is a **well-architected, progressively-implemented code intelligence platform** with a unique value proposition: making code *behavior* queryable rather than just code *structure*.

**Strengths**:
- Coherent vision with honest gap tracking
- Solid DuckDB + Parquet foundation
- Effect model is genuinely novel
- MCP-first architecture is forward-thinking
- **JSX/A11y/WCAG extraction complete** (Phases 0-2)
- **Hook-based validation automation complete** (Phase 3)
- **CLI v4.0 reorganization complete** ‚Äî significantly improved usability
- **Browser automation suite released** (v0.2.0) ‚Äî E2E validation capability
- **47 ADRs** documenting all significant decisions

**Challenges**:
- Actors and OTel remain unimplemented (Phase 4-5)
- Vision docs may set unrealistic expectations for advanced features
- Limited adoption signals
- **Scale benchmarking needed** ‚Äî competitors have proven 400k+ file support
- **Language coverage limited** ‚Äî 3 languages vs Aider's 40+
- **No background agents** ‚Äî unlike Cursor/Windsurf/Copilot

### 6.2 Verdict (Updated)

**Should you invest in DevAC?**

- **Yes, if** you need queryable code intelligence for AI assistants, architecture documentation, or cross-repo analysis
- **Yes, if** you want A11y/WCAG validation integrated with your code analysis (unique capability)
- **Yes, if** you're building React apps and want queryable component hierarchy
- **Yes, if** you want proactive validation via Claude Code hooks (Phase 3 complete)
- **Yes, if** you want a clean CLI experience (sync/status/query model)
- **Wait, if** you need runtime correlation via OTel (Phase 4A)
- **Wait, if** you need XState/Actor discovery (Phase 5)
- **No, if** you only need code search (use Sourcegraph) or build orchestration (use Nx)
- **No, if** you need 40+ language support immediately (use Aider's repo map)
- **No, if** you need background/autonomous agents (use Cursor/Copilot)

### 6.3 Final Score (Updated)

| Dimension | Previous | Current | Notes |
|-----------|----------|---------|-------|
| Vision coherence | 8/10 | **8/10** | Clear, consistent, future-proof |
| Documentation quality | 8/10 | **8.5/10** ‚Üë | 47 ADRs, excellent CLI reorganization docs |
| Implementation depth | 8/10 | **8.5/10** ‚Üë | CLI v4.0, browser automation |
| Competitive position | 7.5/10 | **7.5/10** | WCAG + hooks unique; market evolving fast |
| Adoption readiness | 7.5/10 | **8/10** ‚Üë | 3-command CLI significantly improves UX |
| **Overall** | **8/10** | **8.5/10** ‚Üë | Strong execution, cleaner UX, clear roadmap |

---

## Sources

### Code Analysis Tools
- [FalkorDB Code Graph](https://www.falkordb.com/blog/code-graph/)
- [code-graph-rag on GitHub](https://github.com/vitali87/code-graph-rag)
- [Joern Documentation](https://docs.joern.io/)
- [Joern vs CodeQL Comparison](https://elmanto.github.io/posts/sast_derby_joern_vs_codeql)
- [Code Property Graph Specification](https://cpg.joern.io/)

### AI Coding Assistants (Updated 2026-01-21)
- [GitHub Copilot What's New](https://github.com/features/copilot/whats-new) ‚Äî Agent Mode, Skills, GPT-5.1
- [Cursor Changelog](https://cursor.com/changelog) ‚Äî $29.3B valuation, Cursor 2.0, Composer model
- [Windsurf Editor](https://windsurf.com/editor) ‚Äî Cascade AI, SWE-1.5 model
- [Claude Code GitHub](https://github.com/anthropics/claude-code) ‚Äî Agent SDK, session teleportation
- [Devin AI](https://cognition.ai/) ‚Äî $10.2B valuation, 25% of own PRs

### Code Intelligence & Indexing
- [SCIP Code Intelligence Protocol](https://github.com/sourcegraph/scip)
- [SCIP Announcement](https://sourcegraph.com/blog/announcing-scip)
- [Tree-sitter Code Navigation](https://tree-sitter.github.io/tree-sitter/4-code-navigation.html)
- [Tree-sitter vs LSP Discussion](https://news.ycombinator.com/item?id=18349488)

### Architecture & Diagrams
- [Structurizr](https://structurizr.com/)
- [C4 Model](https://c4model.com/)
- [Diagrams as Code 2.0](https://dev.to/simonbrown/diagrams-as-code-2-0-82k)
- [code2flow](https://github.com/scottrogowski/code2flow)

### Monorepo Tools
- [Nx Dependency Graph](https://nx.dev/docs/features/explore-graph)
- [Nx Affected Analysis](https://nx.dev/blog/ci-affected-graph)
- [Top Monorepo Tools 2025](https://www.aviator.co/blog/monorepo-tools/)

### MCP (Model Context Protocol) ‚Äî Updated 2026-01-21
- [MCP Specification 2025](https://modelcontextprotocol.io/specification/2025-11-25)
- [MCP Impact 2025 - Thoughtworks](https://www.thoughtworks.com/en-us/insights/blog/generative-ai/model-context-protocol-mcp-impact-2025)
- [One Year of MCP](http://blog.modelcontextprotocol.io/posts/2025-11-25-first-mcp-anniversary/)
- [Why MCP Won](https://thenewstack.io/why-the-model-context-protocol-won/) ‚Äî 8M+ downloads, AAIF governance

### Documentation Generation
- [AI Code Documentation - IBM](https://www.ibm.com/think/insights/ai-code-documentation-benefits-top-tips)
- [AI for Code Documentation - Graphite](https://graphite.com/guides/ai-code-documentation-automation)
- [Top Documentation Generator Tools 2025](https://kodesage.ai/blog/7-documentation-generators)

### Hook-Based Automation
- [Personal Assistant Plugin Analysis](../vision/combine-reliable-context-injection-with-intelligent-instruction-following.md) ‚Äî Internal analysis
- [ADR-0043: Hook-Based Validation Triggering](../adr/0043-hook-based-validation-triggering.md) ‚Äî Implementation decision

### Browser Automation ADRs (Added 2026-01-21)
- [ADR-0035: Browser Element Reference Hybrid Strategy](../adr/0035-browser-element-reference-hybrid-strategy.md)
- [ADR-0036: Browser Session Management Singleton Pattern](../adr/0036-browser-session-management-singleton-pattern.md)
- [ADR-0037: Browser MCP Tool Naming and Schema Conventions](../adr/0037-browser-mcp-tool-naming-and-schema-conventions.md)
- [ADR-0038: Browser Error Handling Strategy](../adr/0038-browser-error-handling-strategy.md)
- [ADR-0039: Browser Automation Test Strategy](../adr/0039-browser-automation-test-strategy.md)

### CLI Reorganization (Added 2026-01-21)
- [ADR-0041: CLI Command Structure (v4.0)](../adr/0041-cli-command-structure.md)
- [ADR-0042: MCP Tool Naming Conventions](../adr/0042-mcp-tool-naming-conventions.md)

### Competitive Analysis (Updated 2026-01-21)
- [GitHub Stack Graphs](https://github.blog/2021-12-09-introducing-stack-graphs/) ‚Äî Petabyte-scale code navigation
- [Sourcegraph Amp](https://sourcegraph.com/amp) ‚Äî Real-time codebase access for LLMs
- [CodeRabbit](https://coderabbit.ai/) ‚Äî AST + code graph for AI code reviews
- [Aider Repo Map](https://aider.chat/docs/repomap.html) ‚Äî Tree-sitter + PageRank context selection
- [TechCrunch Cursor Funding](https://techcrunch.com/2025/11/13/coding-assistant-cursor-raises-2-3b-5-months-after-its-previous-round/)

### State Machine Inference Research
- [ProtocolGPT (2025)](https://arxiv.org/abs/2405.00393) ‚Äî LLM + RAG for FSM inference (>90% precision)
- [AALpy](https://github.com/DES-Lab/AALpy) ‚Äî Active automata learning library (Python)
- [LearnLib](https://learnlib.de/) ‚Äî Industrial-strength automata learning (Java)

### OTel Correlation Research
- [OTel Code Attributes](https://opentelemetry.io/docs/specs/semconv/attributes-registry/code/) ‚Äî Semantic conventions for code correlation
- [Tracetest](https://tracetest.io/) ‚Äî Trace-based E2E testing
- [pytest-opentelemetry](https://pypi.org/project/pytest-opentelemetry/) ‚Äî Test instrumentation for coverage
- [OpenTelemetry in 2025](https://thenewstack.io/observability-in-2025-opentelemetry-and-ai-to-fill-in-gaps/)

---

*This review was initially conducted on 2026-01-17, updated on 2026-01-18 (Phases 0-3 complete), and updated on 2026-01-21 (CLI v4.0, browser automation, competitive landscape refresh). Based on the vivief codebase at packages/devac-* and documentation at docs/*.*
